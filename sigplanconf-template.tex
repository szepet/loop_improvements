\documentclass{sigplanconf}

% The following \documentclass options may be useful:

% preprint       Remove this option only once the paper is in final form.
%  9pt           Set paper in  9-point type (instead of default 10-point)
% 11pt           Set paper in 11-point type (instead of default 10-point).
% numbers        Produce numeric citations with natbib (instead of default
%author/year).
% authorversion  Prepare an author version, with appropriate copyright-space
%text.

\usepackage{amsmath}
\usepackage[english]{babel}
\addto\captionsenglish{\renewcommand{\figurename}{Example}}
\usepackage{listings}  
\usepackage{color}
\usepackage[inline]{enumitem}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{mygrayback}{rgb}{0.9,0.9,0.9}
\lstset{ %
	backgroundcolor=\color{mygrayback},   % choose the background color; you 
	%must
	%add \usepackage{color} or \usepackage{xcolor}; should come as last argument
	basicstyle=\small,        % the size of the fonts that are used for the code
	breakatwhitespace=false,
	breaklines=true, 
	captionpos=b,
	commentstyle=\color{mygreen},    % comment style
	extendedchars=true,              % lets you use 
	%frame=single,	                   % adds a frame 
	keepspaces=true,                 % keeps spaces in 
	keywordstyle=\color{blue},       % keyword style
	language=C++,                 %
	numbers=left,                    % where to put the line-numbers; possible
	%values are (none, left, right)
	numbersep=5pt,                   % how far the line-numbers are from the 
	%code
	numberstyle=\footnotesize\color{mygray}, % the style that is used for the
	%line-numbers
	rulecolor=\color{black},         % if not set, the frame-color may be 
	%changed
	%on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,                % show spaces everywhere adding particular
	%underscores; it overrides 'showstringspaces'
	showstringspaces=false,          % underline spaces within strings only
	showtabs=false,                  % show tabs within strings adding 
	%particular
	%underscores
	stepnumber=1,                    % the step between two line-numbers. If 
	%it's
	%1, each line will be numbered
	stringstyle=\color{mymauve},     % string literal style
	tabsize=2,	                   % sets default tabsize to 2 spaces
	title=\lstname                   % show the filename of files included with
	%\lstinputlisting; also try caption instead of title
}

\newcommand{\cL}{{\cal L}}
\hyphenation{Explo-dedGraph}

\begin{document}
	\lstset{language=C++}
	\special{papersize=8.5in,11in}
	\setlength{\pdfpageheight}{\paperheight}
	\setlength{\pdfpagewidth}{\paperwidth}
	
	\conferenceinfo{EuroLLVM}{April 16--17, 2018, Bristol, England}
	\copyrightyear{2018}
	\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}\reprintprice{\$15.00}
	\copyrightdoi{nnnnnnn.nnnnnnn}
	
	% For compatibility with auto-generated ACM eRights management
	% instructions, the following alternate commands are also supported.
	%\CopyrightYear{2016}
	%\conferenceinfo{CONF'yy,}{Month d--d, 20yy, City, ST, Country}
	%\isbn{978-1-nnnn-nnnn-n/yy/mm}\acmPrice{\$15.00}
	%\doi{http://dx.doi.org/10.1145/nnnnnnn.nnnnnnn}
	
	% Uncomment the publication rights used.
	%\setcopyright{acmcopyright}
	\setcopyright{acmlicensed}  % default
	
	%\setcopyright{rightsretained}
	
	%\titlebanner{banner above paper title}        % These are ignored unless
	%\preprintfooter{short description of paper}   % 'preprint' option 
	%%specified.
	
	\title{Improved Loop Execution Modeling in the Clang Static Analyzer}
	%\titlenote{with optional title note}}
	
	\authorinfo{P\'eter Sz\'ecsi}
	{E\"otv\"os Lor\'and University \\ Budapest, Hungary}
	{peterszecsi95@inf.elte.hu}
	%\authorinfo{Name2 \and Name3\thanks{with optional author note}}
	%{Affiliation2/3}
	%{Email2/3}
	
	\maketitle
	% TODO infrastructure, explodedgraph example, Eval, reference
	\begin{abstract}
		The LLVM Clang Static Analyzer is a source code analysis tool which 
		aims to 
		find bugs in C,
		C++, and Objective-C programs using symbolic execution, i.e. it 
		simulates the
		possible execution paths of the code. Currently the simulation of the 
		loops is
		somewhat naive (but efficient), unrolling the loops a predefined 
		constant 
		number of times. However, this approach can result in a loss of 
		coverage in 
		various cases.
		
		This study aims to introduce two alternative approaches which can 
		extend the current method and can be applied simultaneously: 
		\begin{enumerate*} [label={(\arabic*)}, noitemsep]
			\item determining loops worth to fully unroll with applied 
			heuristics, and
			\item using a widening mechanism to simulate an arbitrary number of 
			iteration steps.
		\end{enumerate*}
		These methods were evaluated on numerous open source projects, and 
		proved to increase coverage in most of the cases. This work also laid 
		the
		infrastructure for future loop modeling improvements.
	\end{abstract}
	
	% 2012 ACM Computing Classification System (CSS) concepts
	% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
	
	% \ccsdesc[500]{Software and its engineering~General programming languages}
	% \ccsdesc[300]{Theory of computation~Program analysis}
	% end generated code
	
	% Legacy 1998 ACM Computing Classification System categories are also
	% supported, but not recommended.
	%\category{CR-number}{subcategory}{third-level}[fourth-level]
	%\category{D.3.0}{Programming Languages}{General}
	%\category{F.3.2}{Logics and Meanings of Programs}{Semantics of Programming
	%Languages}[Program analysis]
	
	\keywords
	static analysis, symbolic execution, loop modeling
	
	\section{Introduction}
	The Clang Static Analyzer finds bugs by performing a symbolic execution on 
	the code. During symbolic execution, the program is being interpreted, on a
	function-by-function basis, without any knowledge about the runtime 
	environment. It builds up and traverses an inner model of the execution 
	paths, 
	called \texttt{ExplodedGraph}, for each analyzed function.
	
	An important technical note is that the building of the 
	\texttt{ExplodedGraph} is 
	based on the \texttt{Control Flow Graph} (\texttt{CFG}) of the functions. 
	The 
	\texttt{CFG} represents a source-level, intra-procedural control flow of a 
	statement. This statement can potentially be an entire function body, or 
	just a 
	single expression. The \texttt{CFG} consists of \texttt{CFGBlocks} which 
	are 
	simply containers of statements. The \texttt{CFGBlocks}s essentially 
	represent 
	the basic blocks of the code but can contain some extra custom information. 
	Although basic blocks and \texttt{CFGBlocks} are technically different, in 
	the 
	rest of the article we will use the term basic blocks for 
	\texttt{CFGBlocks} 
	as well for the sake of easier understanding and better illustration.
	
	During the analysis -- based on the function CFGs -- we build up an 
	\texttt{ExplodedGraph}. A 
	node of this graph (called \texttt{ExplodedNode}) contains a 
	\texttt{ProgramPoint} (which determines the location) and a \texttt{State} 
	(which contains the known information at that point). Its paths from the 
	root 
	to the leaves are modeling the different execution paths of the analyzed 
	function. Whenever the execution encounters a branch, a corresponding 
	branch 
	will be created in the \texttt{ExplodedGraph} during the simulated 
	interpretation. 
	Hence, branches lead to an exponential number of \texttt{ExplodedNode}s.
	This combinatorial explosion is handled in the Static Analyzer by stopping 
	the analysis when given conditions are fulfilled. Terminating the analysis 
	process may cause loss of potential true positive results, but it is 
	indispensable for maintaining a reasonable resource consumption regarding 
	the 
	memory and CPU usage. 
	
	These conditions are modeled by the concept of budget.
	The budget is a collection of limitations on the shape of the 
	\texttt{ExplodedGraph}.
	These limitations include:
	\begin{enumerate}  
		\item The maximum number of traversed nodes in the \texttt{Explo-} 
		\texttt{dedGraph}. 
		If 
		this
		number is reached then the analysis of the simulated function stops.
		\item The size of the simulated call stack. When a function call is 
		reached
		then the analysis continues in its body as if it was inlined to the 
		place of
		call (interprocedural). There are several heuristics that may control 
		the	behavior of inlining process. For example the too large functions 
		are 
		not	inlined at all, and the really short functions are not counted in 
		the 
		size of	call stack.
		\item The number of times a function is inlined. The idea behind this
		constraint is that the more a function is analyzed, the less likely it 
		is that a 	bug will appear in it. If this number is reached then that 
		function will not be inlined again in this \texttt{ExplodedGraph}.
		\item The number of times a basic block is processed during the 
		analysis. This	constraint limits the number of loop iterations. When 
		this 
		threshold is reached the currently analyzed execution path will be 
		aborted.
		The budget expression can be used in two ways. Sometimes it means the
		collection of the limitations above, sometimes it refers to one of these
		limitations. This will always be distinguishable from the context.
	\end{enumerate}
	
	
	\section{Motivation}
	Currently, the analyzer handles loops quite simply. More precisely, it 
	unrolls 
	them 4 times by default and then cuts the analysis of the path where the 
	loop 
	would have been unrolled more than 4 times. This behavior is reached by the 
	above presented basic block visiting budget.
	
	Loss in code coverage is one of the problems with this approach to loop
	modeling. Specifically, in cases where the loop is statically known to make 
	more than 4 steps, the analyzer do not analyze the code following the loop. 
	Thus, the naive loop handling (described above) could lead to entirely 
	unchecked code.
	Listing \ref{UnrollMot} shows a small example for that.
	
	\lstset{language=C,caption={Since the loop condition is known at every 
			iteration, the analyzer will not check the 'rest of the function' 
			part in the 
			current state.},label=UnrollMot}
	\begin{lstlisting}
	void foo() {
	int arr[6];
	for (int i = 0; i < 6; i++) {
	arr[i] = i;
	}
	/*rest of the function*/
	}\end{lstlisting}
	
	According to the budget rule concerning the basic block visit number, the
	analysis of the loop stops in the fourth iteration even if the loop 
	condition is simple enough to see that unrolling the whole loop would not 
	be 
	too much extra work relatively. Running out of the budget implies (in this 
	case) that the rest of the function body remains unanalyzed, which may lead 
	to 
	not finding potential bugs.
	Another problem can be seen on Listing \ref{WidenMot}:
	\lstset{language=C++,caption={The loop condition is unknown but the 
			analyzer 
			will not generate a simulation path where n $\ge$ 4 (which can 
			result coverage 
			loss).},label=WidenMot}
	\begin{lstlisting}
	int num();
	void foo() {
	int n = 0;
	for (int i = 0; i < num(); ++i) {
	++n;
	}
	/*rest of the function, n < 4 */
	}\end{lstlisting}
	
	This code fragment results in an analysis which keeps track of the values 
	of 
	\texttt{n} and \texttt{i} variables (this information is stored in the 
	State). 
	In every iteration of the loop the values are updated accordingly. Note 
	that 
	updating the
	State means that a new node is inserted to the \texttt{ExplodedGraph} with 
	the 
	new 
	values. Since the body of the \texttt{num()} function is unknown, the 
	analyzer can not find out its return value. Thus it is considered as 
	unknown. This circumstance
	makes the graph split into two branches. The first one belongs to the 
	symbolic
	execution of the loop body assuming that the loop condition is true. The 
	other one simulates the case where the condition is false and the execution 
	continues after the loop. This process is done for every loop iteration, 
	however, the 4th time assuming the condition is true, the path will be cut 
	short according to the budget rule.
	Although the analyzer generates paths to simulate the code after the loop 
	in the above described case, yet the value of variable \texttt{n} will be 
	always less than 4 on these paths and the rest of the function will only be 
	checked assuming this constraint. This can result in coverage loss as well, 
	since the analyzer will ignore the paths where n is more than 4.
	
	\section{Proposed Solution}
	%The current loop handling method in the Clang Static Analyzer is too 
	%strict.
	In this section two solutions are presented to resolve the previously 
	mentioned
	limitations on symbolic execution of loops in the Clang Static Analyzer. It 
	is important to note that these enhancements are incremental in the sense 
	that on examples which are too complex to handle at the moment, we fall 
	back to 
	the original method. For the sake of simplicity in the following examples a 
	"division by zero" will illustrate the bug we intend to find.
	
	\subsection{Loop Unrolling Heuristics}
	Loop unrolling means we have identified heuristics and patterns (such as 
	loops
	with small number of branches and small known static bound) in order to find
	specific loops which are worth to be completely unrolled. This idea is 
	inspired by the following example:
	
	\lstset{language=C++,caption={Complete unrolling of the loop makes it 
	possible 
			to find the division by zero error.},label=UnrollEx}
	\begin{lstlisting}
	void foo() {
	for (int i = 0; i < 6; i++) {
	/*simple loop which does not
	change 'i' or split the state*/
	}
	int k = 0;
	int l = 2/k; // Division by zero
	}\end{lstlisting}
	
	In the current solution a loop has to fulfill the following conditions in 
	order 
	to be unrolled:
	\begin{enumerate}  
		\item The loop condition should arithmetically compare a variable -- 
		which
		is known at the beginning of the loop -- to a literal (like: 
		\texttt{i~<~6} or
		\texttt{6~$\ge$~i})
		\item The loop should change the loop variable only once in its body 
		and the	difference needs to be constant. (This way the maximum number 
		of 
		steps can be estimated.)
		\item The loop should change the loop variable only once in its body 
		and the	difference needs to be constant. (This way the maximum number 
		of 
		steps can be estimated.)
		\item The estimated number of steps should be less than 128. 
		(Simulating 
		loops which takes thousands of steps because they could single handedly 
		exhaust the budget.)
		\item The loop must not generate new branches or use \texttt{goto} 
		statements.
	\end{enumerate}
	
	By using this method, the bug on the Listing \ref{UnrollEx} example can be 
	found successfully.
	
	\subsection{Loop Widening}
	The final aim of widening is quite the same as the unrolling, to increase 
	the coverage of the analysis. However, it reaches it in a very different 
	way. 
	During widening the analyzer simulates the execution of an arbitrary number 
	of iterations. The analyzer already has a widening algorithm, which reaches 
	this behavior by discarding all of the known information before the last 
	step 
	of the loop. So, the analyzer creates the paths for the first 3 steps and 
	simulate them as usual, but the widening (means the invalidating) happens 
	before the 4th step in order to not lose the first precise simulation 
	branches. 
	This way the coverage will be increased, however, this method is disabled 
	by 
	default, since easily can result in too much false positives. Consider the 
	example on Listing \ref{WidenProb}.
	
	\lstset{language=C++,caption={Invalidating every known information (even 
			the ones which are not modified by the loop) can easily result 
			false 	
			positives.},label=WidenProb}
	\begin{lstlisting}
	int num();
	void foo() {
	bool b = true;
	for (int i = 0; i < num(); ++i) {
	/*does not changes 'b'*/
	}
	int n = 0;
	if (b)
	n++;
	n = 1/n; // False positive:
	// Division by zero
	}\end{lstlisting}
	In this case the analyzer will create and check that unfeasible path where 
	the
	variable \texttt{b} is false, so \texttt{n} is not incremented and lead 
	into a division by zero error. Since this execution path would never be 
	performed 
	while running the analyzed program, it is considered a false positive. My 
	aim 
	was to give a more precise approach for widening.
	
	The principles are that we try to continue the analysis after the block 
	visiting budget is exhausted but invalidate the information only on the 
	variables which are possibly modified by the loop e.g. a statement like 
	\texttt{arr[i] = i} where \texttt{i} is the loop variable means that we 
	discard the data on the whole \texttt{arr} array but nothing else).
	For this I developed a solution which checks every possible way in which a 
	variable can be modified in the loop. Then it evaluates these cases and if 
	it 
	encounters a modified variable which cannot be handled by the invalidation 
	process (e.g.: a pointer variable) then the loop will not be widened and we
	return to the conservative method. This mechanism ensures that we do not 
	create 
	nodes containing invalid states.
	This approach helps us to cover cases and find bugs like the one 
	illustrated on 
	Listing \ref{WidenEx}, and still not report false positives which are 
	represented on Listing \ref{WidenProb}.
	
	\lstset{language=C++,caption={Invalidating the information on only the 
	possible 
			changed variables can result higher coverage (while limiting the 
			number of the 
			found false positives).}, label=WidenEx}
	\begin{lstlisting}
	int num();
	void foo() {
	int n = 0;
	for (int i = 0; i < num(); ++i) {
	++n;
	}
	if (n > 4) {
	int k = 0;
	k = 1/k;  // Division by zero error
	}
	}\end{lstlisting}
	
	We find the bug since we invalidate the known informations on variable
	\texttt{n} (and \texttt{i} as well). This cause the analyzer to create a 
	branch
	where it checks the body of the \texttt{if} statement and finds the bug.
	However, this solution has its own limitations when dealing with nested 
	loops. 
	Consider the case on Listing \ref{WidenEx2}.
	\lstset{language=C++,caption={The naive widening method does not handle 
			well 
			the nested loops. In this example the outer loop will not be 
			widened.},label=WidenEx2}
	\begin{lstlisting}
	int num();
	void foo() {
	int n = 0;
	for (int i = 0; i < num(); ++i) {
	++n;
	for (int j = 0; j < 4; ++j) {
	/*body that does not change n*/
	}
	}
	/*rest of the function, n <= 1 */
	}\end{lstlisting}
	
	In this scenario, when the analyzer first step into the outer loop (so, 
	assumes
	that \texttt{i < num()} is true) and encounter the inner loop, then it 
	consumes
	its (own) block visiting budget. (This implies that it will be widened, 
	although
	in this case it means that only the inner loop counter (\texttt{j}) 
	information
	is discarded.) After that we move on to the next iteration, and assume that 
	we
	are on the path where the outer loop condition is true again. Because we 
	already
	exhausted the budget in the previous iteration, the next visit of the first
	basic block of the inner loop (the condition) means that this path will be
	completely cut off and not analyzed. This results that the outer loop will 
	not
	reach the step number where it would been widened. Furthermore, the outer 
	loop
	will not even reach the 3rd step and the 2nd is stopped at in its body as 
	well
	(as described above). This causes the problem, that even though we use the 
	loop widening method, we will analyze the rest of the function with the 
	assumption \texttt{n <= 1}.
	
	In order to deal with the above described nested loop problem, I have 
	implemented a replay mechanism. This means that whenever we encounter an 
	inner 
	loop which already consumed its budget, we replay the analysis process of 
	the 
	current step of the outer loop but performing a widening first. This ensures
	the creation of a path which assumes that the condition is false and 
	simulates 
	the execution after the loop while the possibly changed information are 
	discarded. This way the analyzer will not exclude some feasible path 
	because of the simple loop handling which solves the problem.
	
	Another note to the widening process that it makes sense to analyze the 
	branch 
	where the condition is true with the widened State as well. The example on 
	Listing \ref{WidenNested} shows a case where this is useful.
	
	\lstset{language=C++,caption={The replay mechanism successfully helps us to 
			find the possible error the outer loop.},label=WidenNested}
	\begin{lstlisting}
	int num();
	void foo() {
	int n = 0;
	int i;
	for (i = 0; i < num(); ++i) {
	if (i == 7) {
	break;
	}
	for (int j = 0; j < 4; ++j) {/* */}
	}
	int n = 1 / (7 - k);
	// ^ Possible division by zero
	}\end{lstlisting}
	
	This way the analyzer will produce a path where the value of \texttt{i} is 
	known
	to be 7, so it will be able find the possible division by zero error.
	
	\subsection{Infrastructure improvements}\label{sec:inf}
	The discussed approaches heavily rely on the fact that we are able to 
	perform the following actions:
	\begin{enumerate}
		\item Decide on any \texttt{ExplodedNode} if it simulates a body of a 
		loop or not,
		\item Recognize the entering and exiting point of a loop on the 
		simulated 
		path.
	\end{enumerate}
	
	However, this information was not provided by the analyzer earlier. 
	Considering the 
	lexical nature of the loops, their entrance and exit points can 
	unambiguously 
	fit into the \texttt{CFG}. 
	
	The \texttt{ProgramPoint} provides the \texttt{LocationContext} which 
	implements a stack data structure for having the information on the 
	different locations of the code.  This implies that the callstack can be 
	represented via this structure in a straightforward manner (and is 
	contained by the \texttt{LocationContext}).
	Since storing the currently simulated loops fits into a stack data 
	structure as well, this information -- called \texttt{LoopContext} -- 
	understandably was implemented as the part of the \texttt{LocationContext} 
	too instead of having them in the \texttt{Store}. Both the \texttt{Store} 
	and the \texttt{LocationContext} are part of an \texttt{ExplodedNode} in 
	form of a pointer. However, these structures use copy-on-write semantics, 
	and the \texttt{LocationContext} changes way less times, this decision 
	saves us memory. 
	
	\section{Evaluation}
	
	The effect of the described loop modeling approaches was measured on 
	various 
	C/C++ open source projects. These are the following:
	
	\begin{tabular}{ |c||c|c| }
		\hline
		Project & LoC & Language \\
		\hline
		TinyXML & 20k & C++ \\
		\hline
		Curl & 21k & C  \\ 
		\hline
		Redis & 40k & C \\ 
		\hline
		Xerces & 228k & C++ \\ 
		\hline
		Vim & 540k & C \\ 
		\hline
		OpenSSL & 550k & C  \\ 
		\hline
		PostgreSQL & 950k & C \\ 
		\hline
		FFmpeg & 1080k & C \\	
		\hline
	\end{tabular}
	
	\subsection{Coverage and the number of explored paths}
	These statisics are produced by the analyzer (mÃ¡r benne van, ezt valahogy 
	ertelmesen elmondani, hogy ezert valid es nem en szoptam ossze az eterbol). 
	The 
	coverage percentage is based on the ratio of the visited and the total 
	number 
	of basic blocks in the analyzed functions, which results in a small 
	imprecision. 
	It is important to note that the introduced loop modeling methods require 
	having 
	additional loop entrance and exit point information (described in section 
	\ref{sec:inf}) in the \texttt{CFG}. This can lead to having more 
	basic blocks in the \texttt{CFG} and it can affect the statistics. As a 
	result, 
	even statistics produced by using the current loop modeling 
	approach were measured with this information added to the \texttt{CFG}.
	
	The coverage and the number of explored paths are generated for every 
	translation unit and then summarized. This means that header files which 
	are included in more than one translation unit can influence more 
	statistics.
	However, we use this summarization process consistently for every 
	measurement, 
	this way the results reflect the reality. 
	
	The tables presented in this section summarize measurement results using 
	different loop modeling approaches: the current practice (denoted by Normal)
	and the hereby introduced loop unrolling (Unroll) and loop widening (Widen)
	methods separately and simultaneously (U+W).
	
	\begin{table}[!htb]
		\centering
		\begin{tabular}{ |c||c|c|c|c| } 
			\hline
			Project & Normal & Unroll & Widen & U + W \\
			\hline \hline
			TinyXML & 84.2 & 84.2 & 85.1 & 85.1 \\ 
			\hline
			Curl & 76.2 & 76.9 & 77.7 & 77.2 \\ 
			\hline
			Redis & 68.5 & 69.1 & 68.5 & 71.3 \\ 
			\hline
			Xerces & 92.3 & 92.4 & 92.7 & 92.7 \\ 
			\hline
			Vim & 60.4 & 60.6 &	60.6 & 60.7 \\ 
			\hline
			OpenSSL & 97.4 & 97.5 & 97.7 & 97.7 \\ 
			\hline
			PostgreSQL & 76.9 &	77.0 & 76.9 & 76.9 \\ 
			\hline
			FFmpeg & 86.1 & 86.3 & 87.0 & 86.8 \\	
			\hline
		\end{tabular}
		\caption{The code coverage of the analysis on the evaluated projects 
		expressed 
			in percentage.}\label{tab:cov}
	\end{table}
	
	Table \ref{tab:cov} shows the coverage difference using the introduced 
	approaches. On most of the projects we managed to strictly increase 
	analysis coverage 
	using any of the proposed approaches. The widening method had a stronger 
	influence on the coverage in the average case. However, the complete unroll 
	of 
	specific loops could result in a higher coverage as well (e.g. Curl, Redis).
	In general, enabling both of them was the most beneficial in respect of the 
	coverage.
	
	\begin{table}[!htb]
		\centering
		\begin{tabular}{ |c||r|r|r|r| } 
			\hline
			Project & Normal & Unroll & Widen & U + W \\
			\hline \hline
			TinyXML & 14452 & 15460 & 14765 &	15773  \\ 
			\hline
			Curl & 18272 & 18577 & 28835 & 24279 \\ 
			\hline
			Redis & 69857 & 70097 & 98446 & 100929 \\ 
			\hline
			Xerces & 395615 & 398077 & 430989 &	433358 \\ 
			\hline
			Vim & 155451 & 157266 & 188136 & 173121 \\ 
			\hline
			OpenSSL & 687175 & 687932 & 700464 & 701013 \\ 
			\hline
			PostgreSQL & 382660 & 383874 & 453188 & 419118  \\ 
			\hline
			FFmpeg & 466613 & 458480 & 571399 & 521725  \\ 		
			\hline
		\end{tabular}
		\caption{The numbers of explored execution paths using different loop 
		modeling
			approaches.}\label{tab:pathnum}
	\end{table}
	Table \ref{tab:pathnum} presents the numbers of analyzed execution paths.
	As expected, both introduced loop modeling methods resulted in a higher 
	number of
	simulated paths on (almost) all of the projects. The only exception is the 
	unrolling 
	approach on the FFmpeg project, which caused the budget limiting the number 
	of 
	traversed \texttt{ExplodedNode}s to exhaust earlier, slightly decreasing 
	the number of checked paths. Enabling both of the features resulted similar 
	or less number of explored than the runs using only widening. It was the 
	
	\subsection{Found bugs}
	
	\begin{table}[!htb]
		\centering
		\begin{tabular}{ |c||c|c|c|c| } 
			\hline
			Project & Normal & Unroll & Widen & U + W \\
			\hline \hline
			TinyXML & 1 & 1 & 3 & 3 \\
			\hline
			Curl & 16 & 16 & 16 & 16 \\ 
			\hline
			Redis & 55 & 58 & 55 & 59 \\ 
			\hline
			Xerces & 62 & 62 & 61 & 61 \\ 
			\hline
			Vim & 74 & 74 & 76 & 78 \\
			\hline
			OpenSSL & 152 & 152 & 153 & 153 \\ 
			\hline
			PostgreSQL & 323 & 323 & 327 & 331 \\ 
			\hline
			FFmpeg & 425 & 420 & 423 & 454 \\ 		
			\hline
		\end{tabular}
		\caption{The number of bug reports produced by the analyzer.} 
		\label{tab:reportnum}
	\end{table}
	The number of bug reports using the different loop modeling methods can be 
	seen
	on Table \ref{tab:reportnum}. The increase in analysis coverage and number 
	of checked path usually implies that the number of the found bugs will be 
	increased as well. This can be observed on these numbers as well, however,
	the increase rate of the found bugs is higher. But if we figyelembe 
	vesszuk, that how much more paths we simulated.
	\subsection{Analysis time}
	
	\begin{table}[!htb]
		\centering
		\begin{tabular}{ |c||c|c|c|c| } 
			\hline
			Project & Normal & Unroll & Widen & U + W \\
			\hline \hline
			TinyXML & 0:51 & 0:51 & 0:52 & 0:52 \\ 
			\hline
			Curl & 0:50 & 1:06 & 0:55 & 1:10 \\ 
			\hline
			Redis & 2:06 & 2:11 & 2:28 & 2:10 \\ 
			\hline
			Xerces & 3:38 & 3:34 & 3:37 & 3:39 \\ 
			\hline
			Vim & 3:11 & 3:26 & 3:18 & 3:27 \\ 
			\hline
			OpenSSL & 2:04 & 2:22 & 2:13 & 2:19 \\ 
			\hline
			PostgreSQL & 7:03 & 8:32 & 7:48 & 7:59 \\ 
			\hline
			FFmpeg & 9:40 & 10:22 & 10:14 & 11:20 \\ 		
			\hline
		\end{tabular}
		\caption{Average measured time of the analysis expressed in minutes. 
			(Average of 5 runs.)}\label{tab:time}
	\end{table}
	The running time on the different projects are represented on Table 
	\ref{tab:time}.
	
	\section{Conclusion}
	
	We introduced two alternative approaches for simulating loops during 
	symbolic 
	execution. These were implemented and subsequently evaluated on various 
	open 
	source projects, with a clear improvement of code coverage in general. With 
	the 
	help of the new methods we are able to explore previously skipped, 
	feasible  
	execution paths, especially when both of them are used in conjunction.
	
	The required changes done to the underlying infrastructure should also ease 
	the 
	implementation of future enhancements. In particular, information tracked 
	by the 
	analysis about location contexts were expanded with additional fields.
	While code coverage was measured to have increased by an average of *x*, 
	there 
	also was a noticeable performance penalty. In some cases, we observed a 
	runtime 
	increase of *y*, with an average of ... . The number of simulated paths 
	also 
	increased proportionally with the time taken, suggesting this time was well 
	spent.
	
	\section{Future work}
	The heuristic patterns for completely unrolled loops could be extended to 
	involve loops whose bound is a known variable which is not changed in the 
	body. 
	Furthermore, even more general rules would be beneficial: consider loops 
	where  
	the value variables are known at the beginning and they are affected by a 
	known 
	constant change by every iteration. These improvements have not been 
	implemented
	yet due to some technical and framework limitations.
	
	During the widening process we invalidate any possibly changed information. 
	However, a change made on a pointer could mean that we need to 
	invalidate all variables due to the lack of advanced pointer analysis. 
	Therefore, 
	introducing pointer analysis algorithms to the analyzer could help to 
	develop a 
	more precise invalidation process.
	
	The infrastructural improvements enable the analyzer to provide entry 
	points 
	for bug finding modules (checkers) on loop entrances/exits and identify the 
	currently simulated loop for every \texttt{ExplodedNode}. On top of these 
	entry 
	points new checkers can be implemented.
	
	\acks
	I would like to thank my main mentor of GSoC, Artem Dergachev for his 
	advices and guidance on getting to know the core of the Static Analyzer. 
	Furthermore, I would like to thank the members of the CodeChecker team from 
	Ericsson for their valueable and useful suggestions on the paper. 
	%especially for my honoured and best friend, master of C++ and all 
	%information sciences, Tibor Brunner. 
	
	% The 'abbrvnat' bibliography style is recommended.
	
	\bibliographystyle{abbrvnat}
	
	% The bibliography should be embedded for final submission.
	
	\begin{thebibliography}{}
		\softraggedright
		
		\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
		P. Q. Smith, and X. Y. Jones. ...reference text...
		
	\end{thebibliography}
	
	
\end{document}

